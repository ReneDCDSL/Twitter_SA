{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afab6b49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:51:47.756583Z",
     "start_time": "2023-11-10T12:51:45.601454Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from time import time\n",
    "## Models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def GridSearch_(X, y, parameters, model, scoring=['accuracy', 'f1', 'precision', 'recall'], refit=\"accuracy\", n_jobs=-1):\n",
    "    '''Perform the grid search analysis with selected parameters set, model and data. Returns the grid search object with the\n",
    "    set of best parameters'''\n",
    "    # Create a pipeline with model\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('model', model),\n",
    "    ])\n",
    "    # Create a grid search object with parameters to test\n",
    "    grid_search = GridSearchCV(pipeline, parameters, scoring=scoring, refit=refit, n_jobs=n_jobs, verbose=1)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print()\n",
    "    print(\"Data length: \", len(y))\n",
    "    print(\"Pipeline:\", ' '.join([str(_) for name, _ in pipeline.steps[:2]]), model)\n",
    "    print()\n",
    "    print(\"Parameters:\")\n",
    "    pprint(parameters)\n",
    "    print()\n",
    "    t0 = time()\n",
    "    grid_search.fit(X, y)\n",
    "    print(\"Duration: %0.1fs (n_jobs: %.f)\" % ((time() - t0), n_jobs))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c68281f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:49:44.482534Z",
     "start_time": "2023-11-10T12:49:44.474647Z"
    }
   },
   "outputs": [],
   "source": [
    "def ratio_positive(user, df):\n",
    "    positive_tweets = df[df[\"user\"] == user]['emotion'].sum()\n",
    "    total_tweets = len(df[df[\"user\"] == user])\n",
    "    return np.round(positive_tweets/total_tweets*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2285cd85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:49:44.991857Z",
     "start_time": "2023-11-10T12:49:44.986596Z"
    }
   },
   "outputs": [],
   "source": [
    "def ratio_positive_all(user, df):\n",
    "    positive_tweets = df[df[\"user\"] == user]['emotion'].sum()\n",
    "    total_tweets = len(df[df[\"user\"] == user])\n",
    "    return positive_tweets, total_tweets, np.round(positive_tweets/total_tweets*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7452be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:49:45.475945Z",
     "start_time": "2023-11-10T12:49:45.467823Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object, metric='accuracy'):\n",
    "    ''' Create dictionary that maps input metric to actual metric name in GridSearchCV'''\n",
    "    \n",
    "    metric_dict = {'precision': 'mean_test_precision',\n",
    "                   'recall': 'mean_test_recall',\n",
    "                   'f1': 'mean_test_f1',\n",
    "                   'accuracy': 'mean_test_accuracy',\n",
    "                   }\n",
    "    \n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "    \n",
    "    # Isolate the row of the df with the max(metric) score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "    \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision],\n",
    "                          'recall': [recall],\n",
    "                          'F1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          },\n",
    "                         )\n",
    "\n",
    "    return table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
